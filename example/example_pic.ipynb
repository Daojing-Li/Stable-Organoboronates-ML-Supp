{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import shap\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "scaler = StandardScaler()\n",
    "data = pd.read_csv('data_round2.csv')\n",
    "def leave_one_out_validation(best_model, scaler, X_model, y):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out validation.\n",
    "\n",
    "    Parameters:\n",
    "    best_model (object): The trained best model.\n",
    "    scaler (object): The standardization scaler.\n",
    "    X_model (DataFrame): The feature data.\n",
    "    y (Series): The target data.\n",
    "\n",
    "    Returns:\n",
    "    r2_loo (float): The R^2 score from leave-one-out validation.\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    y_pred_loo = []\n",
    "    y_true_loo = []\n",
    "    for train_index, test_index in loo.split(X_model):\n",
    "        X_train_loo, X_test_loo = X_model.iloc[train_index], X_model.iloc[test_index]\n",
    "        y_train_loo, y_test_loo = y.iloc[train_index], y.iloc[test_index]\n",
    "        best_model.fit(scaler.fit_transform(X_train_loo), y_train_loo)\n",
    "        y_pred_loo.append(best_model.predict(scaler.transform(X_test_loo))[0])\n",
    "        y_true_loo.append(y_test_loo.values[0])\n",
    "    r2_loo = r2_score(y_true_loo, y_pred_loo)\n",
    "    return r2_loo\n",
    "def plot_model_results(data, model,random_state=42):\n",
    "    features = data.select_dtypes(include=[np.number]).columns\n",
    "    X = data[features].drop('yield', axis=1)\n",
    "    y = data['yield']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    model.fit(scaler.fit_transform(X_train), y_train)\n",
    "    y_pred_train = model.predict(scaler.transform(X_train))\n",
    "    y_pred_test = model.predict(scaler.transform(X_test))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_LOO = leave_one_out_validation(model, scaler, X, y)\n",
    "\n",
    "    r2_test_list = []\n",
    "    mae_test_list = []\n",
    "    model1 = model\n",
    "    for i in range(100):\n",
    "        random_state = i\n",
    "        X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        model1.fit(scaler.fit_transform(X_train_1), y_train_1)\n",
    "        y_pred_test_1 = model1.predict(scaler.transform(X_test_1))\n",
    "        r2_test_1 = r2_score(y_test_1, y_pred_test_1)\n",
    "        mae_test_1 = mean_absolute_error(y_test_1, y_pred_test_1)\n",
    "        mae_test_list.append(mae_test_1)\n",
    "        r2_test_list.append(r2_test_1)\n",
    "\n",
    "\n",
    "    # plt.scatter(y_train, y_pred_train, color='blue', label='Train')\n",
    "    # plt.scatter(y_test, y_pred_test, color='green', label='Test')\n",
    "    # plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', label='Perfect Prediction')\n",
    "    # plt.xlabel('Actual Yield')\n",
    "    # plt.ylabel('Predicted Yield')\n",
    "    # plt.title(f'Actual vs. Predicted Yield(C:{model.C:.4f},epsilon:{model.epsilon:.4f})')\n",
    "    # plt.legend()\n",
    "    # plt.grid(False)\n",
    "\n",
    "    # plt.text(100, -5, f\"Mean MAE:{np.mean(mae_test_list):.2f}\", fontsize=12, ha='right', va='bottom')   \n",
    "    # plt.text(100, 0, f\"MAE: {mae_test:.2f}\", fontsize=12, ha='right', va='bottom')\n",
    "    # plt.text(100, 5, f\"RMSE: {rmse_test:.2f}\", fontsize=12, ha='right', va='bottom')\n",
    "    # plt.text(100, 10, f\"R2: {r2_test:.2f}\", fontsize=12, ha='right', va='bottom')\n",
    "    # plt.text(100, 15, f\"R2_LOO:{r2_LOO:.2f}\", fontsize=12, ha='right', va='bottom')                                                                                                                                                                            \n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.scatter(range(100), r2_test_list, s=100)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.yticks([-1, 0, 1])\n",
    "    plt.xlabel('Random Test Set Index')\n",
    "    plt.ylabel('R2')\n",
    "    plt.title(f'R2 on 100 Random Test Sets (C:{model.C:.4f},epsilon:{model.epsilon:.4f})')\n",
    "    plt.show()\n",
    "\n",
    "def plot_actual_vs_predicted(data, model, X_train=None, X_test=None, y_train=None, y_test=None, random_state=None, font_size=14, font_position=(100, 0)):\n",
    "    \"\"\"\n",
    "    Plot the actual vs predicted yield using the given data and model.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input data.\n",
    "    model: The trained model.\n",
    "    X_train (DataFrame, optional): The feature data for training. If not provided, data will be split randomly.\n",
    "    X_test (DataFrame, optional): The feature data for testing. If not provided, data will be split randomly.\n",
    "    y_train (Series, optional): The target data for training. If not provided, data will be split randomly.\n",
    "    y_test (Series, optional): The target data for testing. If not provided, data will be split randomly.\n",
    "    random_state (int, optional): The random state for data splitting. If not provided, a random state will be used.\n",
    "    font_size (int, optional): The font size for the text on the plot. Default is 14.\n",
    "    font_position (tuple, optional): The position of the text on the plot. Default is (100, 0).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if X_train is None or X_test is None or y_train is None or y_test is None:\n",
    "        # Split the data randomly if not provided\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.drop('yield', axis=1), data['yield'], test_size=0.2, random_state=random_state)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)  # Fit the model\n",
    "\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_LOO = leave_one_out_validation(model, scaler, data.drop('yield', axis=1), data['yield'])\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.scatter(y_train, y_pred_train, color='blue', label='Train')\n",
    "    plt.scatter(y_test, y_pred_test, color='green', label='Test' )\n",
    "    plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', label='Perfect Prediction')\n",
    "    plt.xlabel('Experimental Yield', fontsize=font_size)\n",
    "    plt.ylabel('Predicted Yield', fontsize=font_size)\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.text(font_position[0], font_position[1], fr\"$MAE_{{test}}: {mae_test:.2f}$\", fontsize=font_size, ha='right', va='bottom')\n",
    "    plt.text(font_position[0], font_position[1]+8, fr\"$RMSE_{{test}}: {rmse_test:.2f}$\", fontsize=font_size, ha='right', va='bottom')\n",
    "    plt.text(font_position[0], font_position[1]+16, fr\"$R^2_{{LOO}}:{r2_LOO:.2f}$\", fontsize=font_size, ha='right', va='bottom')                                                                                                                                                                            \n",
    "    plt.text(font_position[0], font_position[1]+24, fr\"$Pearson \\; R_{{test}}: {np.sqrt(r2_test):.4f}$\", fontsize=font_size, ha='right', va='bottom')\n",
    "    plt.text(font_position[0], font_position[1]+32, fr\"$Pearson \\; R_{{train}}: {np.sqrt(r2_train):.4f}$\", fontsize=font_size, ha='right', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_and_plot_with_unnormal_values(model_name, y_train, y_pred_train, y_test, y_pred_test, X_train, X_test, threshold=30):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model and plot the actual vs. predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    model_name (str): Name of the model.\n",
    "    y_train (array-like): Actual target values for the training set.\n",
    "    y_pred_train (array-like): Predicted target values for the training set.\n",
    "    y_test (array-like): Actual target values for the test set.\n",
    "    y_pred_test (array-like): Predicted target values for the test set.\n",
    "    X_train (DataFrame): Features for the training set. Defaults to None.\n",
    "    X_test (DataFrame): Features for the test set. Defaults to None.\n",
    "    threshold (float, optional): Threshold for identifying anomalies. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate performance metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "    # Identify anomalies based on the threshold\n",
    "    anomalies_train = np.abs(y_pred_train - y_train) > threshold\n",
    "    anomalies_test = np.abs(y_pred_test - y_test) > threshold\n",
    "\n",
    "    anomaly_indices_train = np.where(anomalies_train)[0]\n",
    "    anomaly_indices_test = np.where(anomalies_test)[0]\n",
    "\n",
    "    # Create DataFrames to store anomaly details\n",
    "    results_train_df = pd.DataFrame(columns=['Actual', 'Predicted'] + list(X_train.columns))\n",
    "    results_test_df = pd.DataFrame(columns=['Actual', 'Predicted'] + list(X_train.columns))\n",
    "\n",
    "    # Populate DataFrames with anomaly details\n",
    "    for idx in anomaly_indices_train:\n",
    "        new_row = pd.DataFrame({\n",
    "            'Actual': [y_train.iloc[idx]],\n",
    "            'Predicted': [y_pred_train[idx]],\n",
    "            **{col: [X_train.iloc[idx][col]] for col in X_train.columns}\n",
    "        })\n",
    "        results_train_df = pd.concat([results_train_df, new_row], ignore_index=True)\n",
    "\n",
    "    for idx in anomaly_indices_test:\n",
    "        new_row = pd.DataFrame({\n",
    "            'Actual': [y_test.iloc[idx]],\n",
    "            'Predicted': [y_pred_test[idx]],\n",
    "            **{col: [X_test.iloc[idx][col]] for col in X_test.columns}\n",
    "        })\n",
    "        results_test_df = pd.concat([results_test_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Display anomaly details\n",
    "    display(results_train_df)\n",
    "    display(results_test_df)\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train')\n",
    "    plt.scatter(y_test, y_pred_test, color='green', alpha=0.6, label='Test')\n",
    "    plt.scatter(y_train[anomalies_train], y_pred_train[anomalies_train], color='orange', label='Anomalies Train')\n",
    "    plt.scatter(y_test[anomalies_test], y_pred_test[anomalies_test], color='red', label='Anomalies Test')\n",
    "    plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', linewidth=2)  # Draw diagonal line\n",
    "\n",
    "    # Annotate anomalies on the plot\n",
    "    for i in range(len(results_train_df)):\n",
    "        plt.annotate(f'{i}', (results_train_df.iloc[i, 0], results_train_df.iloc[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    for i in range(len(results_test_df)):\n",
    "        plt.annotate(f'{i}', (results_test_df.iloc[i, 0], results_test_df.iloc[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs Predicted Values for {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Display performance metrics on the plot\n",
    "    plt.gca().text(1.05, 0, f'Train RMSE: {rmse_train:.4f}\\nTrain R^2: {r2_train:.4f}\\nTrain MAE: {mae_train:.4f}\\n\\nTest RMSE: {rmse_test:.4f}\\nTest R^2: {r2_test:.4f}\\nTest MAE: {mae_test:.4f}',\n",
    "                  transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "data_pic = pd.read_csv('model_pic_round3.csv')\n",
    "\n",
    "# Keep only Baseline, XGBOOST, Lasso, SVR, RandomForest rows in data_pic, delete the rest\n",
    "data_pic = data_pic[data_pic['Model'].isin(['AdaBoost', 'RF',  'XGBoost','SVR','MLP','KNR'])]\n",
    "x = data_pic['Model']\n",
    "y1 = data_pic['MAE_mean']\n",
    "y2 = data_pic['R2_LOO']\n",
    "\n",
    "x_pos = np.arange(len(x))\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Get paired colors\n",
    "colors = sns.color_palette(\"Paired\")\n",
    "\n",
    "# Plot the first set of data as bars\n",
    "bar1 = ax1.bar(x_pos - 0.2, y1, 0.4, label='MAE_mean', color=colors[0])\n",
    "\n",
    "# Set y-axis label\n",
    "ax1.set_ylabel(r'MAE$_{mean}$',fontsize = 16)\n",
    "ax1.set_ylim(15, 35)\n",
    "# ax1.set_ylim(15, 35)\n",
    "# Set ax1 y-axis ticks with interval of 5\n",
    "ax1.set_yticks(np.arange(15, 36, 5))\n",
    "# Format ax1 y-axis tick labels as integers\n",
    "ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\n",
    "ax1.tick_params(axis='y',labelsize=16)\n",
    "ax1.tick_params(axis='y',labelsize=16)\n",
    "# ax1.legend(loc='upper right',bbox_to_anchor=(1, 0.9))\n",
    "labels = [r'$MAE_{mean}$', r'$R^2_{LOO}$']\n",
    "\n",
    "# Create a second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "\n",
    "min_y2 = 0\n",
    "max_y2 = 0.6\n",
    "print(min_y2)\n",
    "# Set the range of the second y-axis, starting from the minimum value\n",
    "\n",
    "# Plot the second set of data as bars\n",
    "bar2 = ax2.bar(x_pos + 0.2, y2,0.4, label=r'$R^2_{LOO}$', color=colors[1],bottom=0)\n",
    "ax1.legend([bar1, bar2], labels, loc='upper left',fontsize = 13)\n",
    "\n",
    "# Set y-axis label\n",
    "ax2.set_ylabel(r'$R^2_{LOO}$',fontsize = 16)\n",
    "# ax2.legend(loc='upper right',bbox_to_anchor=(1, 1))\n",
    "ax2.set_ylim(min_y2, max_y2)\n",
    "ax2.tick_params(axis='y',labelsize=16)\n",
    "# Get the minimum and maximum values of y2 data\n",
    "\n",
    "# Set x-axis labels\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(x,fontsize = 16)  # Add rotated labels to improve display\n",
    "\n",
    "# Set chart title\n",
    "plt.title(\"Performance of selected models\",fontsize = 16)\n",
    "\n",
    "# Display the figure\n",
    "# plt.show()\n",
    "plt.savefig('/home/ldj/Code/Projects/Stable-Organoboronates-ML-Supp/model_pic_filtered.png',dpi=600,format='png',bbox_inches='tight')\n",
    "plt.savefig('/home/ldj/Code/Projects/Stable-Organoboronates-ML-Supp/model_pic_filtered.eps',format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('data_round2.csv')\n",
    "model1 = SVR(C= 63.91726535154404, epsilon= 0.01666549751396613, gamma= 5.275057481551831)\n",
    "data1 = data[['Mulliken_charge_H_H', 'APT_charge_H_H', 'NICS1', 'delta_G_B', 'lumo_B','yield']]\n",
    "\n",
    "features = data1.select_dtypes(include=[np.number]).columns\n",
    "X = data1[features].drop('yield', axis=1)\n",
    "y = data1['yield']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 100))\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "model1.fit(X_train_scaled, y_train_scaled)\n",
    "# Analyze SVR model using SHAP\n",
    "explainer = shap.KernelExplainer(model1.predict, X_train_scaled)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "# Plot bar chart of SHAP analysis\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Calculate mean_abs_shap_values\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Radar chart\n",
    "# Define feature names in LaTeX format\n",
    "\n",
    "latex_feature_names = [\n",
    "    r'$Mulliken \\ Charge \\ H_{Ar-H}$',\n",
    "    r'$APT \\ Charge \\ H_{Ar-H}$',\n",
    "    r'$NICS(Ⅰ)$',\n",
    "    r'$\\Delta G_{B}$',\n",
    "    r'$LUMO_{B}$'\n",
    "]\n",
    "\n",
    "# Change order\n",
    "# Define new order\n",
    "new_order = [4,1,2,3,0]  # e.g., delta_G_B, lumo_B, Mulliken, APT, NICS1\n",
    "\n",
    "# Reorder latex_feature_names\n",
    "latex_feature_names = [latex_feature_names[i] for i in new_order]\n",
    "\n",
    "# Reorder mean_abs_shap_values\n",
    "mean_abs_shap_values = np.array([mean_abs_shap_values[i] for i in new_order])\n",
    "\n",
    "\n",
    "\n",
    "# Set radar chart parameters\n",
    "N = len(latex_feature_names)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]  # Close the radar chart\n",
    "\n",
    "# Initialize radar chart, reduce figure size\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Prepare data\n",
    "values = mean_abs_shap_values.tolist()\n",
    "values += values[:1]  # Close radar chart data\n",
    "\n",
    "# Draw radar chart\n",
    "ax.plot(angles, values, linewidth=2.5, linestyle='solid', label='Feature Importance')\n",
    "ax.fill(angles, values, alpha=0.3)\n",
    "\n",
    "# Set radar chart labels, increase font size\n",
    "ax.set_xticks(angles[:-1])\n",
    "labels = ax.set_xticklabels(latex_feature_names, fontsize=18)\n",
    "\n",
    "# Adjust label positions\n",
    "for i, label in enumerate(labels):\n",
    "    angle = angles[i]\n",
    "    # Adjust label alignment based on angle\n",
    "    if 0 <= angle < pi/2 or 3*pi/2 < angle <= 2*pi:\n",
    "        label.set_horizontalalignment('left')\n",
    "    else:\n",
    "        label.set_horizontalalignment('right')\n",
    "    \n",
    "    # Fine-tune vertical alignment\n",
    "    if pi/4 <= angle < 3*pi/4 or 5*pi/4 <= angle < 7*pi/4:\n",
    "        label.set_verticalalignment('center')\n",
    "    elif 3*pi/4 <= angle < 5*pi/4:\n",
    "        label.set_verticalalignment('top')\n",
    "    else:\n",
    "        label.set_verticalalignment('bottom')\n",
    "\n",
    "# Hide radial ticks\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "# Hide radial ticks\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "fig.suptitle('Feature importance based on SHAP values', fontsize=20, fontname='Arial', y=0.88) \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.title('Feature Importance based on SHAP Values', fontsize=18, y=1.1)\n",
    "# plt.title('Feature Importance based on SHAP Values', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/ldj/Code/Projects/Stable-Organoboronates-ML-Supp/round2_output/model1/radar_chart_dpi600.tif', dpi=600, format='tif', bbox_inches='tight')\n",
    "plt.savefig('/home/ldj/Code/Projects/Stable-Organoboronates-ML-Supp/round2_output/model1/radar_chart_dpi600.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot r2 on 100 random samples for top 5 best SVR(after feature filter)\n",
    "def plot_r2_on_100_random_samples(data, model, model_name):\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    # 创建输出文件夹\n",
    "    output_dir = 'output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    features = data.select_dtypes(include=[np.number]).columns\n",
    "    X = data[features].drop('yield', axis=1)\n",
    "    y = data['yield']\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    mae_test_list = []\n",
    "    for i in range(100):\n",
    "        random_state = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        model.fit(scaler.fit_transform(X_train), y_train)\n",
    "        y_pred_test = model.predict(scaler.transform(X_test))\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        mae_test_list.append(mae_test)\n",
    "    mae_mean = np.mean(mae_test_list)\n",
    "\n",
    "    y_pred_train = model.predict(scaler.transform(X_train))\n",
    "    y_pred_test = model.predict(scaler.transform(X_test))\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    # 设置纵横比\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.scatter(y_train, y_pred_train, color='blue', label='Train')\n",
    "    plt.scatter(y_test, y_pred_test, color='green', label='Test')\n",
    "    plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', label='Perfect Prediction')\n",
    "    plt.xlabel('True Values/(%)')\n",
    "    plt.ylabel('Predicted Values/(%)')\n",
    "    plt.axis('square')\n",
    "    # plt.title(f'Actual vs Predicted Values for SVR')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.text(0.95, 0.05, fr\"$Pearson \\; R_{{train}}: {np.sqrt(r2_train):.4f}$\", fontsize=12, ha='right', va='bottom', fontname='Arial', transform=plt.gca().transAxes)\n",
    "    plt.text(0.95, 0.19, fr\"$RMSE_{{test}}: {rmse_test:.4f}$\", fontsize=12, ha='right', va='bottom', fontname='Arial', transform=plt.gca().transAxes)\n",
    "    plt.text(0.95, 0.12, fr\"$Pearson \\; R_{{test}}: {np.sqrt(r2_test):.4f}$\", fontsize=12, ha='right', va='bottom', fontname='Arial', transform=plt.gca().transAxes)        \n",
    "    plt.text(0.95, 0.26, fr\"$MAE_{{test}}: {mae_test:.4f}$\", fontsize=12, ha='right', va='bottom', fontname='Arial', transform=plt.gca().transAxes)\n",
    "    plt.text(0.95, 0.33, fr'$MAE_{{mean}}: {mae_mean:.4f}$', fontsize=12, ha='right', va='bottom', fontname='Arial', transform=plt.gca().transAxes)\n",
    "    plt.savefig(f'output/{model_name}_scatter.tif', dpi=600)\n",
    "    plt.savefig(f'output/{model_name}_scatter.png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    r2_test_list = []\n",
    "    mae_test_list = []\n",
    "    for i in range(100):\n",
    "        random_state = i\n",
    "        X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        model.fit(scaler.fit_transform(X_train_1), y_train_1)\n",
    "        y_pred_test_1 = model.predict(scaler.transform(X_test_1))\n",
    "        r2_test_1 = r2_score(y_test_1, y_pred_test_1)\n",
    "        mae_test_1 = mean_absolute_error(y_test_1, y_pred_test_1)\n",
    "        mae_test_list.append(mae_test_1)\n",
    "        r2_test_list.append(r2_test_1)\n",
    "    plt.figure(figsize=(17, 3))\n",
    "    plt.scatter(range(100), r2_test_list, s=100)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.yticks([-1, 0, 1])\n",
    "    plt.xlabel('Random Test Set Index', fontsize=20, fontname='Arial')\n",
    "    plt.ylabel(fr'$R^2$', fontsize=20, fontname='Arial')\n",
    "    plt.title(fr'$R^2$ on 100 Random Test Sets (SVR: C={float(model.get_params()[\"C\"]):.4f}, epsilon={float(model.get_params()[\"epsilon\"]):.4f}, kernel={(model.get_params()[\"kernel\"])}),  $R^2_{{mean}}$: {float(np.mean(r2_test_list)):.4f}', fontsize=20, fontname='Arial')\n",
    "    plt.savefig(f'output/{model_name}_r2.tif', dpi=600)\n",
    "    plt.savefig(f'output/{model_name}_r2.png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    # 保存模型\n",
    "    joblib.dump(model, f'output/{model_name}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
